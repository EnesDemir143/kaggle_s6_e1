{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Student Test Score Prediction\n",
    "\n",
    "**Kaggle Playground Series S6E1** - Complete notebook with EDA, 4 models ensemble.\n",
    "\n",
    "| Feature | Value |\n",
    "|---------|-------|\n",
    "| **Models** | CatBoost, LightGBM, XGBoost, MLP |\n",
    "| **CV** | ‚ùå Train/Val Split (90/10) |\n",
    "| **Optuna** | ‚úÖ 50 trials per model |\n",
    "| **Early Stopping** | ‚úÖ 100 rounds |\n",
    "| **GPU** | ‚úÖ Enabled |\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. Setup & Imports\n",
    "2. Data Loading\n",
    "3. **Exploratory Data Analysis (EDA)**\n",
    "   - Dataset Overview\n",
    "   - Missing Values & Data Types\n",
    "   - Target Distribution\n",
    "   - Numeric Features Analysis\n",
    "   - Categorical Features Analysis\n",
    "   - Correlation Analysis\n",
    "4. Preprocessing Functions\n",
    "5. Optuna Search Spaces\n",
    "6. Model Training\n",
    "7. Results & Visualization\n",
    "8. Ensemble & Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Constants & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PATHS (Kaggle)\n",
    "# =============================================================================\n",
    "TRAIN_PATH = \"/kaggle/input/playground-series-s6e1/train.csv\"\n",
    "TEST_PATH = \"/kaggle/input/playground-series-s6e1/test.csv\"\n",
    "SAMPLE_SUB = \"/kaggle/input/playground-series-s6e1/sample_submission.csv\"\n",
    "\n",
    "# =============================================================================\n",
    "# COLUMNS\n",
    "# =============================================================================\n",
    "NUMERIC_COLS = [\"age\", \"study_hours\", \"class_attendance\", \"sleep_hours\"]\n",
    "CAT_COLS = [\"gender\", \"course\", \"internet_access\", \"sleep_quality\", \n",
    "            \"study_method\", \"facility_rating\", \"exam_difficulty\"]\n",
    "TARGET = \"exam_score\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIG\n",
    "# =============================================================================\n",
    "SEED = 42\n",
    "VAL_SIZE = 0.1\n",
    "N_TRIALS = 50\n",
    "EARLY_STOPPING = 100\n",
    "MODELS = [\"catboost\", \"lightgbm\", \"xgboost\", \"mlp\"]\n",
    "\n",
    "print(f\"üìä Config: {N_TRIALS} Optuna trials, {VAL_SIZE:.0%} val split, SEED={SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"üîπ Train: {train_df.shape[0]:,} rows √ó {train_df.shape[1]} cols\")\n",
    "print(f\"üîπ Test:  {test_df.shape[0]:,} rows √ó {test_df.shape[1]} cols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 4.1 Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä TRAIN DATA - First 10 Rows\")\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä TEST DATA - First 5 Rows\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Types & Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã DATA TYPES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n{'Column':<20} {'Train Dtype':<15} {'Test Dtype':<15}\")\n",
    "print(\"-\"*50)\n",
    "for col in train_df.columns:\n",
    "    train_dtype = str(train_df[col].dtype)\n",
    "    test_dtype = str(test_df[col].dtype) if col in test_df.columns else \"N/A\"\n",
    "    print(f\"{col:<20} {train_dtype:<15} {test_dtype:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä TRAIN DATA INFO\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "train_missing = train_df.isnull().sum()\n",
    "test_missing = test_df.isnull().sum()\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Train Missing': train_missing,\n",
    "    'Train %': (train_missing / len(train_df) * 100).round(2),\n",
    "    'Test Missing': test_missing,\n",
    "    'Test %': (test_missing / len(test_df) * 100).round(2)\n",
    "})\n",
    "\n",
    "print(\"üîç MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "if missing_df['Train Missing'].sum() == 0 and missing_df['Test Missing'].sum() == 0:\n",
    "    print(\"‚úÖ No missing values in train or test data!\")\n",
    "else:\n",
    "    display(missing_df[missing_df['Train Missing'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà NUMERIC FEATURES - Statistical Summary\")\n",
    "train_df[NUMERIC_COLS + [TARGET]].describe().T.style.format(\"{:.2f}\").background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù CATEGORICAL FEATURES - Unique Values\")\n",
    "cat_summary = pd.DataFrame({\n",
    "    'Column': CAT_COLS,\n",
    "    'Unique Values': [train_df[col].nunique() for col in CAT_COLS],\n",
    "    'Sample Values': [train_df[col].unique()[:3].tolist() for col in CAT_COLS]\n",
    "})\n",
    "display(cat_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(train_df[TARGET], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(train_df[TARGET].mean(), color='red', linestyle='--', label=f'Mean: {train_df[TARGET].mean():.2f}')\n",
    "axes[0].axvline(train_df[TARGET].median(), color='orange', linestyle='--', label=f'Median: {train_df[TARGET].median():.2f}')\n",
    "axes[0].set_xlabel('Exam Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Target Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(train_df[TARGET], vert=True)\n",
    "axes[1].set_ylabel('Exam Score')\n",
    "axes[1].set_title('Target Box Plot')\n",
    "\n",
    "# KDE\n",
    "train_df[TARGET].plot(kind='kde', ax=axes[2], color='steelblue', linewidth=2)\n",
    "axes[2].set_xlabel('Exam Score')\n",
    "axes[2].set_title('Target Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('target_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Target Statistics:\")\n",
    "print(f\"   Range: {train_df[TARGET].min():.1f} - {train_df[TARGET].max():.1f}\")\n",
    "print(f\"   Mean: {train_df[TARGET].mean():.2f}\")\n",
    "print(f\"   Std: {train_df[TARGET].std():.2f}\")\n",
    "print(f\"   Skewness: {train_df[TARGET].skew():.3f}\")\n",
    "print(f\"   Kurtosis: {train_df[TARGET].kurtosis():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Numeric Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(NUMERIC_COLS):\n",
    "    # Histogram\n",
    "    axes[i].hist(train_df[col], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[i].axvline(train_df[col].mean(), color='red', linestyle='--', alpha=0.7)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'{col} Distribution')\n",
    "    \n",
    "    # Box plot\n",
    "    axes[i+4].boxplot(train_df[col], vert=True)\n",
    "    axes[i+4].set_ylabel(col)\n",
    "    axes[i+4].set_title(f'{col} Box Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('numeric_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Categorical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cats = len(CAT_COLS)\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, 10))\n",
    "\n",
    "for i, col in enumerate(CAT_COLS):\n",
    "    value_counts = train_df[col].value_counts()\n",
    "    bars = axes[i].bar(range(len(value_counts)), value_counts.values, color=colors[:len(value_counts)], alpha=0.8)\n",
    "    axes[i].set_xticks(range(len(value_counts)))\n",
    "    axes[i].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].set_title(f'{col} Distribution')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = len(train_df)\n",
    "    for bar, val in zip(bars, value_counts.values):\n",
    "        pct = val / total * 100\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                    f'{pct:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('categorical_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Target vs Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(NUMERIC_COLS):\n",
    "    axes[i].scatter(train_df[col], train_df[TARGET], alpha=0.3, s=5, color='steelblue')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(train_df[col], train_df[TARGET], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(train_df[col].min(), train_df[col].max(), 100)\n",
    "    axes[i].plot(x_line, p(x_line), 'r--', linewidth=2, label='Trend')\n",
    "    \n",
    "    corr = train_df[col].corr(train_df[TARGET])\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(TARGET)\n",
    "    axes[i].set_title(f'{col} vs {TARGET} (r={corr:.3f})')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('target_vs_numeric.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Target vs Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(CAT_COLS):\n",
    "    order = train_df.groupby(col)[TARGET].mean().sort_values(ascending=False).index\n",
    "    sns.boxplot(data=train_df, x=col, y=TARGET, order=order, ax=axes[i], palette='viridis')\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
    "    axes[i].set_title(f'{TARGET} by {col}')\n",
    "\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('target_vs_categorical.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numeric features\n",
    "numeric_df = train_df[NUMERIC_COLS + [TARGET]]\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=0.5, ax=ax,\n",
    "            annot_kws={'size': 12})\n",
    "ax.set_title('Correlation Matrix (Numeric Features + Target)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target correlations\n",
    "target_corr = corr_matrix[TARGET].drop(TARGET).sort_values(ascending=False)\n",
    "print(\"üéØ Feature Correlations with Target:\")\n",
    "print(\"=\"*40)\n",
    "for feat, corr in target_corr.items():\n",
    "    bar = \"‚ñà\" * int(abs(corr) * 30)\n",
    "    sign = \"+\" if corr > 0 else \"-\"\n",
    "    print(f\"{feat:<20} {sign}{bar} {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 Train vs Test Distribution Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(NUMERIC_COLS + CAT_COLS[:4]):\n",
    "    if col in NUMERIC_COLS:\n",
    "        axes[i].hist(train_df[col], bins=30, alpha=0.5, label='Train', color='blue', density=True)\n",
    "        axes[i].hist(test_df[col], bins=30, alpha=0.5, label='Test', color='orange', density=True)\n",
    "    else:\n",
    "        train_counts = train_df[col].value_counts(normalize=True).sort_index()\n",
    "        test_counts = test_df[col].value_counts(normalize=True).sort_index()\n",
    "        \n",
    "        x = np.arange(len(train_counts))\n",
    "        width = 0.35\n",
    "        axes[i].bar(x - width/2, train_counts.values, width, label='Train', alpha=0.7)\n",
    "        axes[i].bar(x + width/2, test_counts.values, width, label='Test', alpha=0.7)\n",
    "        axes[i].set_xticks(x)\n",
    "        axes[i].set_xticklabels(train_counts.index, rotation=45, ha='right')\n",
    "    \n",
    "    axes[i].set_title(f'{col} Distribution')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('train_vs_test_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Train and Test distributions look similar - no significant distribution shift detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.12 EDA Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä EDA SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "üîπ Dataset Size: {len(train_df):,} train, {len(test_df):,} test\n",
    "üîπ Features: {len(NUMERIC_COLS)} numeric, {len(CAT_COLS)} categorical\n",
    "üîπ Target Range: {train_df[TARGET].min():.1f} - {train_df[TARGET].max():.1f}\n",
    "üîπ Missing Values: None ‚úÖ\n",
    "\n",
    "üìà Key Insights:\n",
    "   ‚Ä¢ study_hours has the strongest positive correlation with exam_score\n",
    "   ‚Ä¢ class_attendance also shows moderate positive correlation\n",
    "   ‚Ä¢ No severe outliers detected in numeric features\n",
    "   ‚Ä¢ Categorical features are well-balanced\n",
    "   ‚Ä¢ Train/Test distributions are consistent\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, model, fit=True, fitted_objects=None, target=None):\n",
    "    \"\"\"\n",
    "    Model-specific preprocessing.\n",
    "    \n",
    "    - CatBoost: Native categorical handling\n",
    "    - LightGBM: Category dtype\n",
    "    - XGBoost: Ordinal encoding\n",
    "    - MLP: StandardScaler + OneHotEncoder\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Drop id column\n",
    "    if ID_COL in df.columns:\n",
    "        df = df.drop(columns=[ID_COL])\n",
    "    \n",
    "    fitted_objects = fitted_objects or {}\n",
    "    \n",
    "    if model == \"catboost\":\n",
    "        fitted_objects[\"cat_features\"] = CAT_COLS\n",
    "        \n",
    "    elif model == \"lightgbm\":\n",
    "        for col in CAT_COLS:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "        fitted_objects[\"cat_features\"] = CAT_COLS\n",
    "        \n",
    "    elif model == \"xgboost\":\n",
    "        if fit:\n",
    "            encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "            df[CAT_COLS] = encoder.fit_transform(df[CAT_COLS])\n",
    "            fitted_objects[\"encoder\"] = encoder\n",
    "        else:\n",
    "            df[CAT_COLS] = fitted_objects[\"encoder\"].transform(df[CAT_COLS])\n",
    "            \n",
    "    elif model == \"mlp\":\n",
    "        if fit:\n",
    "            scaler = StandardScaler()\n",
    "            df[NUMERIC_COLS] = scaler.fit_transform(df[NUMERIC_COLS])\n",
    "            fitted_objects[\"scaler\"] = scaler\n",
    "            \n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "            encoded = encoder.fit_transform(df[CAT_COLS])\n",
    "            fitted_objects[\"encoder\"] = encoder\n",
    "        else:\n",
    "            df[NUMERIC_COLS] = fitted_objects[\"scaler\"].transform(df[NUMERIC_COLS])\n",
    "            encoded = fitted_objects[\"encoder\"].transform(df[CAT_COLS])\n",
    "        \n",
    "        encoded_df = pd.DataFrame(\n",
    "            encoded, \n",
    "            columns=fitted_objects[\"encoder\"].get_feature_names_out(CAT_COLS), \n",
    "            index=df.index\n",
    "        )\n",
    "        df = df.drop(columns=CAT_COLS)\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "    \n",
    "    return df, fitted_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Optuna Search Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catboost_params(trial):\n",
    "    \"\"\"CatBoost hyperparameter search space.\"\"\"\n",
    "    return {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 500, 3000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-3, 10.0, log=True),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "        \"random_seed\": SEED,\n",
    "        \"early_stopping_rounds\": EARLY_STOPPING,\n",
    "        \"verbose\": 0,\n",
    "        \"task_type\": \"GPU\",\n",
    "    }\n",
    "\n",
    "def get_lightgbm_params(trial):\n",
    "    \"\"\"LightGBM hyperparameter search space.\"\"\"\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 3000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 256),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"random_state\": SEED,\n",
    "        \"verbose\": -1,\n",
    "        \"device\": \"gpu\",\n",
    "    }\n",
    "\n",
    "def get_xgboost_params(trial):\n",
    "    \"\"\"XGBoost hyperparameter search space.\"\"\"\n",
    "    return {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 3000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 5.0, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"random_state\": SEED,\n",
    "        \"early_stopping_rounds\": EARLY_STOPPING,\n",
    "        \"verbosity\": 0,\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "    }\n",
    "\n",
    "def get_mlp_params(trial):\n",
    "    \"\"\"MLP hyperparameter search space.\"\"\"\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
    "    hidden_sizes = tuple(\n",
    "        trial.suggest_int(f\"neurons_l{i}\", 32, 256) \n",
    "        for i in range(n_layers)\n",
    "    )\n",
    "    return {\n",
    "        \"hidden_layer_sizes\": hidden_sizes,\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"]),\n",
    "        \"solver\": \"adam\",\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-5, 1e-1, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [128, 256, 512, 1024]),\n",
    "        \"learning_rate_init\": trial.suggest_float(\"learning_rate_init\", 1e-4, 1e-2, log=True),\n",
    "        \"max_iter\": 500,\n",
    "        \"early_stopping\": True,\n",
    "        \"validation_fraction\": 0.1,\n",
    "        \"n_iter_no_change\": 20,\n",
    "        \"random_state\": SEED,\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "SEARCH_SPACES = {\n",
    "    \"catboost\": get_catboost_params,\n",
    "    \"lightgbm\": get_lightgbm_params,\n",
    "    \"xgboost\": get_xgboost_params,\n",
    "    \"mlp\": get_mlp_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"Calculate regression metrics.\"\"\"\n",
    "    return {\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def create_model(model_name, params):\n",
    "    \"\"\"Create model instance with given parameters.\"\"\"\n",
    "    if model_name == \"catboost\":\n",
    "        return CatBoostRegressor(**params)\n",
    "    elif model_name == \"lightgbm\":\n",
    "        return LGBMRegressor(**params)\n",
    "    elif model_name == \"xgboost\":\n",
    "        return XGBRegressor(**params)\n",
    "    elif model_name == \"mlp\":\n",
    "        return MLPRegressor(**params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "def fit_model(model, model_name, X_train, y_train, X_val, y_val, fitted_objects):\n",
    "    \"\"\"Fit model with appropriate early stopping strategy.\"\"\"\n",
    "    if model_name == \"catboost\":\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            cat_features=fitted_objects.get(\"cat_features\", CAT_COLS),\n",
    "            use_best_model=True,\n",
    "            verbose=0,\n",
    "        )\n",
    "    elif model_name == \"lightgbm\":\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=EARLY_STOPPING, verbose=False),\n",
    "                lgb.log_evaluation(period=0),\n",
    "            ],\n",
    "        )\n",
    "    elif model_name == \"xgboost\":\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False,\n",
    "        )\n",
    "    elif model_name == \"mlp\":\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_with_optuna(model_name, X_train, y_train, X_val, y_val, fitted_objects):\n",
    "    \"\"\"Train model with Optuna hyperparameter optimization.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Training {model_name.upper()} with Optuna ({N_TRIALS} trials)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = SEARCH_SPACES[model_name](trial)\n",
    "        model = create_model(model_name, params)\n",
    "        model = fit_model(model, model_name, X_train, y_train, X_val, y_val, fitted_objects)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        return rmse\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True, n_jobs=1)\n",
    "    \n",
    "    print(f\"‚úÖ Best trial: #{study.best_trial.number}, RMSE: {study.best_value:.4f}\")\n",
    "    \n",
    "    # Retrain with best params\n",
    "    best_params = SEARCH_SPACES[model_name](study.best_trial)\n",
    "    best_model = create_model(model_name, best_params)\n",
    "    best_model = fit_model(best_model, model_name, X_train, y_train, X_val, y_val, fitted_objects)\n",
    "    \n",
    "    return best_model, study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = train_df.drop(columns=[TARGET])\n",
    "y = train_df[TARGET]\n",
    "\n",
    "# Train/val split (NO CV!)\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=VAL_SIZE, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"üìä Train: {len(X_train_raw):,} | Val: {len(X_val_raw):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "trained_models = {}\n",
    "all_test_preds = {}\n",
    "metrics_results = []\n",
    "\n",
    "for model_name in MODELS:\n",
    "    # Preprocess - fit on train only!\n",
    "    X_train, fitted_objects = preprocess(\n",
    "        X_train_raw.copy(), model=model_name, fit=True, target=y_train\n",
    "    )\n",
    "    X_val, _ = preprocess(\n",
    "        X_val_raw.copy(), model=model_name, fit=False, fitted_objects=fitted_objects\n",
    "    )\n",
    "    \n",
    "    # Train with Optuna\n",
    "    model, best_params, best_rmse = train_with_optuna(\n",
    "        model_name, X_train, y_train, X_val, y_val, fitted_objects\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    metrics = evaluate(y_val.values, y_pred_val)\n",
    "    metrics[\"model\"] = model_name\n",
    "    metrics_results.append(metrics)\n",
    "    \n",
    "    print(f\"üìà Val Metrics: RMSE={metrics['rmse']:.4f}, MAE={metrics['mae']:.4f}, R¬≤={metrics['r2']:.4f}\")\n",
    "    \n",
    "    # Predict on test\n",
    "    X_test_raw = test_df.copy()\n",
    "    test_ids = X_test_raw[ID_COL].values\n",
    "    X_test, _ = preprocess(\n",
    "        X_test_raw, model=model_name, fit=False, fitted_objects=fitted_objects\n",
    "    )\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Store\n",
    "    trained_models[model_name] = model\n",
    "    all_test_preds[model_name] = test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(metrics_results)\n",
    "results_df = results_df[[\"model\", \"rmse\", \"mae\", \"r2\"]]\n",
    "results_df = results_df.sort_values(\"rmse\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "display(results_df.style.format({\n",
    "    \"rmse\": \"{:.4f}\",\n",
    "    \"mae\": \"{:.4f}\",\n",
    "    \"r2\": \"{:.4f}\",\n",
    "}).highlight_min(subset=[\"rmse\", \"mae\"], color=\"lightgreen\")\n",
    " .highlight_max(subset=[\"r2\"], color=\"lightgreen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "metrics_names = [\"rmse\", \"mae\", \"r2\"]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(MODELS)))\n",
    "\n",
    "for ax, metric in zip(axes, metrics_names):\n",
    "    values = results_df[metric].values\n",
    "    bars = ax.bar(results_df[\"model\"], values, color=colors, alpha=0.8)\n",
    "    ax.set_title(f\"{metric.upper()}\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_ylabel(metric.upper())\n",
    "    \n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                f\"{val:.4f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions vs Actuals for best model\n",
    "best_model_name = results_df.iloc[0][\"model\"]\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "# Get validation predictions\n",
    "X_val_best, fitted_best = preprocess(X_val_raw.copy(), model=best_model_name, fit=True, target=y_train)\n",
    "y_pred_best = best_model.predict(X_val_best)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_val.values, y_pred_best, alpha=0.3, s=5)\n",
    "min_val = min(y_val.min(), y_pred_best.min())\n",
    "max_val = max(y_val.max(), y_pred_best.max())\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "axes[0].set_xlabel(\"Actual\")\n",
    "axes[0].set_ylabel(\"Predicted\")\n",
    "axes[0].set_title(f\"Predictions vs Actual ({best_model_name})\")\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_val.values - y_pred_best\n",
    "axes[1].scatter(y_pred_best, residuals, alpha=0.3, s=5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel(\"Predicted\")\n",
    "axes[1].set_ylabel(\"Residual\")\n",
    "axes[1].set_title(\"Residuals vs Predicted\")\n",
    "\n",
    "# Residual histogram\n",
    "axes[2].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[2].axvline(x=0, color='r', linestyle='--')\n",
    "axes[2].set_xlabel(\"Residual\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_title(\"Residual Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"prediction_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Feature Importance (Tree Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "tree_models = [\"catboost\", \"lightgbm\", \"xgboost\"]\n",
    "\n",
    "for ax, model_name in zip(axes, tree_models):\n",
    "    model = trained_models[model_name]\n",
    "    \n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        feature_names = NUMERIC_COLS + CAT_COLS\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)\n",
    "        \n",
    "        ax.barh(range(len(indices)), importances[indices], alpha=0.8, color='steelblue')\n",
    "        ax.set_yticks(range(len(indices)))\n",
    "        ax.set_yticklabels([feature_names[i] for i in indices])\n",
    "        ax.set_title(f\"{model_name.upper()}\", fontsize=12, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Importance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Stacking Ensemble with RidgeCV Meta-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# =============================================================================\n",
    "# STACKING CONFIG\n",
    "# =============================================================================\n",
    "STACKING_ALPHAS = (0.001, 0.01, 0.1, 1.0, 10.0, 100.0)\n",
    "STACKING_CV = 5\n",
    "\n",
    "print(\"üîß Stacking Config:\")\n",
    "print(f\"   Meta-learner: RidgeCV\")\n",
    "print(f\"   Alphas: {STACKING_ALPHAS}\")\n",
    "print(f\"   CV folds: {STACKING_CV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect validation predictions for stacking (out-of-fold predictions)\n",
    "print(\"\\nüìä Collecting base model predictions for stacking...\")\n",
    "\n",
    "val_preds_stack = {}\n",
    "for model_name in MODELS:\n",
    "    # Preprocess validation data with fitted objects from training\n",
    "    X_train_temp, fitted_temp = preprocess(\n",
    "        X_train_raw.copy(), model=model_name, fit=True, target=y_train\n",
    "    )\n",
    "    X_val_temp, _ = preprocess(\n",
    "        X_val_raw.copy(), model=model_name, fit=False, fitted_objects=fitted_temp\n",
    "    )\n",
    "    \n",
    "    # Get validation predictions\n",
    "    val_pred = trained_models[model_name].predict(X_val_temp)\n",
    "    val_preds_stack[model_name] = val_pred\n",
    "    print(f\"   {model_name}: {len(val_pred):,} predictions\")\n",
    "\n",
    "# Build stacking features (base model predictions as features)\n",
    "X_stack_val = np.column_stack([val_preds_stack[m] for m in MODELS])\n",
    "X_stack_test = np.column_stack([all_test_preds[m] for m in MODELS])\n",
    "\n",
    "print(f\"\\n‚úÖ Stacking features shape: Val={X_stack_val.shape}, Test={X_stack_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RidgeCV meta-learner\n",
    "print(\"\\nüöÄ Training RidgeCV Meta-Learner...\")\n",
    "\n",
    "meta_learner = RidgeCV(\n",
    "    alphas=STACKING_ALPHAS,\n",
    "    fit_intercept=True,\n",
    "    cv=STACKING_CV,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    ")\n",
    "\n",
    "meta_learner.fit(X_stack_val, y_val)\n",
    "\n",
    "print(f\"‚úÖ Best alpha: {meta_learner.alpha_}\")\n",
    "print(f\"   Coefficients: {dict(zip(MODELS, meta_learner.coef_.round(4)))}\")\n",
    "print(f\"   Intercept: {meta_learner.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate stacking on validation\n",
    "stacking_val_pred = meta_learner.predict(X_stack_val)\n",
    "stacking_metrics = evaluate(y_val.values, stacking_val_pred)\n",
    "\n",
    "print(f\"\\nüìà Stacking Validation Metrics:\")\n",
    "print(f\"   RMSE: {stacking_metrics['rmse']:.4f}\")\n",
    "print(f\"   MAE:  {stacking_metrics['mae']:.4f}\")\n",
    "print(f\"   R¬≤:   {stacking_metrics['r2']:.4f}\")\n",
    "\n",
    "# Compare with individual models\n",
    "print(\"\\nüìä Comparison with Base Models:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Model':<15} {'RMSE':>10} {'MAE':>10} {'R¬≤':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for res in sorted(metrics_results, key=lambda x: x['rmse']):\n",
    "    print(f\"{res['model']:<15} {res['rmse']:>10.4f} {res['mae']:>10.4f} {res['r2']:>10.4f}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'STACKING':<15} {stacking_metrics['rmse']:>10.4f} {stacking_metrics['mae']:>10.4f} {stacking_metrics['r2']:>10.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test predictions with stacking\n",
    "stacking_test_pred = meta_learner.predict(X_stack_test)\n",
    "\n",
    "print(f\"\\nüéØ Stacking Test Predictions:\")\n",
    "print(f\"   Count: {len(stacking_test_pred):,}\")\n",
    "print(f\"   Min: {stacking_test_pred.min():.2f}\")\n",
    "print(f\"   Max: {stacking_test_pred.max():.2f}\")\n",
    "print(f\"   Mean: {stacking_test_pred.mean():.2f}\")\n",
    "print(f\"   Std: {stacking_test_pred.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stacking weights and comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# 1. Model weights (coefficients)\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(MODELS)))\n",
    "bars = axes[0].bar(MODELS, meta_learner.coef_, color=colors, alpha=0.8)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0].set_ylabel(\"Coefficient\")\n",
    "axes[0].set_title(\"RidgeCV Model Weights\", fontweight=\"bold\")\n",
    "for bar, val in zip(bars, meta_learner.coef_):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                f\"{val:.3f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "# 2. RMSE comparison\n",
    "all_rmse = [r['rmse'] for r in sorted(metrics_results, key=lambda x: MODELS.index(x['model']))]\n",
    "all_rmse.append(stacking_metrics['rmse'])\n",
    "model_labels = MODELS + ['STACKING']\n",
    "colors_ext = list(plt.cm.viridis(np.linspace(0.2, 0.8, len(MODELS)))) + ['#ff6b6b']\n",
    "bars = axes[1].bar(model_labels, all_rmse, color=colors_ext, alpha=0.8)\n",
    "axes[1].set_ylabel(\"RMSE\")\n",
    "axes[1].set_title(\"RMSE Comparison\", fontweight=\"bold\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "for bar, val in zip(bars, all_rmse):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                f\"{val:.4f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "# 3. Predictions comparison (first 100 samples)\n",
    "x = np.arange(min(100, len(stacking_test_pred)))\n",
    "for model_name in MODELS:\n",
    "    axes[2].plot(x, all_test_preds[model_name][:100], alpha=0.4, label=model_name, linewidth=1)\n",
    "axes[2].plot(x, stacking_test_pred[:100], 'k-', linewidth=2, label='Stacking')\n",
    "axes[2].set_xlabel(\"Sample Index\")\n",
    "axes[2].set_ylabel(\"Predicted Score\")\n",
    "axes[2].set_title(\"Predictions Comparison\", fontweight=\"bold\")\n",
    "axes[2].legend(loc='upper right', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"stacking_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission with stacking predictions\n",
    "submission = pd.DataFrame({\n",
    "    ID_COL: test_ids,\n",
    "    TARGET: stacking_test_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Submission saved: submission.csv\")\n",
    "print(f\"   Shape: {submission.shape}\")\n",
    "print(f\"   Method: Stacking with RidgeCV (alpha={meta_learner.alpha_})\")\n",
    "\n",
    "submission.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
